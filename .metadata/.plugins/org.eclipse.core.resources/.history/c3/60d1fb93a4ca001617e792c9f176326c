# -*- coding: utf-8 -*-
'''
Created on 24 Ara 2016

@author: Purgoufr
'''

#Genel olarak yaptigimiz olayin adi web scraping
#web scraping: cesitli yazilim ve metotlar ile hedef web sitelerinden icerik kopyalama* veya belli bilgileri alma islemine verilen isim
# #istedi�in URL i a�ma
# import webbrowser
# webbrowser.open('http://inventwithpython.com/')

#----------------------------------------------------------------------------------------
# #ornek program kopyaladigin adresi google maps te uygulamayi calistirdiginda acar(Ctrl+C yap ve run a bas)
# import webbrowser, sys, pyperclip
# if len(sys.argv) > 1:
#     # Get address from command line.
#     address = ' '.join(sys.argv[1:])
# else:
#     # Get address from clipboard.
#     address = pyperclip.paste()
# 
# webbrowser.open('https://www.google.com/maps/place/' + address)

#----------------------------------------------------------------------------------------
#URL to download (internetten text indirdik)
#len(res.text) ile text dosyasinin boyutunu ogrendik
#printlen(res.text[:500]) yaparak 500 karakterlik kismini yazdirdik dosyada 178.000 karakter var

# import requests 
# res = requests.get('https://automatetheboringstuff.com/files/rj.txt')
# type(res)
# #res.status_code == requests.codes.ok    #kod calismadi(bu kodda istek yaptigimiz link durumunu sorguladik(true or false?)
# print(res.raise_for_status())  #linki kontrol eder problem varsa hata verir
# print(len(res.text))
# print(res.text[:500])

#----------------------------------------------------------------------------------------
# #Linkteki sorunu debugla bulalim
# #request.get ile linki yazdik ve res.raise_for_status() ile linki kontrol ettik
# 
# import requests
# res = requests.get('http://inventwithpython.com/page_that_does_not_exist')
# try:
#     res.raise_for_status()
# except Exception as exc:
#     print('There was a problem: %s' % (exc))

#----------------------------------------------------------------------------------------
#Dosyayi bilgisayara kaydetme
#Note: write binary mode yani wb. Metnin Unicode kodlamasini korumak icin metin(string)
#verileri yerine ikili veri(binary) yazmaniz gerekir.
#open komuyla text dosyasi olusturduk
#response objesini donguye almak icin for loop icinde iter_content() kullandik
#playFile.write ile text dosyasina request ettigimiz icerigi yazdik
# import requests
# res = requests.get('https://automatetheboringstuff.com/files/rj.txt')
# res.raise_for_status()
# playFile = open('C:\\Users\\Purgoufr\\Documents\\Eclipse Projects\\Python\\Web_Scraping\\web_test_folder\\RomeoAndJuliet.txt', 'wb')
# for chunk in res.iter_content(100000):
#     playFile.write(chunk)
# playFile.close()

#----------------------------------------------------------------------------------------
# #Parsing HTML with the BeautifulSoup Module(siteden istedigin kismi cekme(istedigin class i cekme parse etme))
# #Parse icin Beautiful Soup modulunu kullanacagiz
# #Bu kodda requests.get() il No Starch Press in ana sayfasini indirmek icin kullandik
# #BeautifulSoup nesnesi noStarchSoup adli bir degiskende depolanir.
# import requests, bs4
# res = requests.get('http://nostarch.com')
# res.raise_for_status()
# noStarchSoup = bs4.BeautifulSoup(res.text)
# print(type(noStarchSoup))
# exampleFile = open('example.html')
# exampleSoup = bs4.BeautifulSoup(exampleFile)
# print(type(exampleSoup))
#     
#----------------------------------------------------------------------------------------
# select() sorgulari
#soup.select('div')          All elements named <div> (<div> adli tum elemanlar)

#soup.select('#author')      The element with an id attribute of author( author id ozellikli tum elemanlar )

#soup.select('.notice')      All elements that use a CSS class attribute named notice(notice adli CSS class ozellikli tum elemanlar):

#soup.select('div span')     All elements named <span> that are within an element named <div>(<div> ve <span> adli tum elemanlar)

#soup.select('div > span')   All elements named <span> that are directly within an element named <div>, with no other
#                            element in between(<Span> adli ve dogrudan <div> adli bir ogenin icinde bulunan ve arasinda
#                            baska bir oge bulunmayan tum ogeler)

#soup.select('input[name]')  All elements named <input> that have a name attribute with any value(Herhangi bir degere sahip 
#                            name ozniteligine sahip <input> isimli tum ogeler)

#soup.select('input[type="button"]')  All elements named <input> that have an attribute named type with value button 
#                                     input adinda buton degerine sahip(buton) ozelligi tasiyan butun elemanlar  
    
#----------------------------------------------------------------------------------------
# #daha once olusturdugumuz example.html dosyasindan author ile ilgili bilgileri getirdik bu bilgiler
#bu ornek program html kodundaki <p> elemanlarini getirir
# import bs4
# exampleFile = open('example.html')
# exampleSoup = bs4.BeautifulSoup(exampleFile.read())
# elems = exampleSoup.select('#author')
# print(type(elems))
# print(len(elems))
# print(type(elems[0]))
# print(elems[0].getText())
# print(str(elems[0]))
# print(elems[0].attrs)
# 
# pElems = exampleSoup.select('p')
# print(str(pElems[0]))
# print(pElems[0].getText())
# print(str(pElems[1]))
# print(pElems[1].getText())
# print(str(pElems[2]))
# print(pElems[2].getText())

#----------------------------------------------------------------------------------------
##bu ornek program html kodundaki <span> elemanlarini getirir
# import bs4
# soup = bs4.BeautifulSoup(open('example.html'))
# spanElem = soup.select('span')[0]
# print(str(spanElem))
# print(spanElem.get('id'))
# print(spanElem.get('some_nonexistent_addr') == None)
# print(spanElem.attrs)
    
#----------------------------------------------------------------------------------------
#ornek program google da birsey aratip birkac farkli linki tek tek yeni sekmede acmadan
#sadece komut satirina aramak istedigin seyi yazacaksin ve onunla ilgili sayfalar web browser da otomatik acilacak
#
#komut satirina yazilanlari okumasi icin sys.argv kullanicaz
#arama sonuclarini cekmek getirmek(fetch) icin request module kullancaz
#her arama sonucu icin linkleri bulacagiz
#webbrowser.open() i kullanarak web sayfalarini acacagiz.
#google arama url i 'https://www.google.com.tr/search?q=' esittirden sonra aranacak kelime gelir

import requests, sys, webbrowser, bs4
import string
print('Googling...') # display text while downloading the Google page
weight = string(raw_input())
res = requests.get('http://google.com/search?q=' + ' '.join(sys.argv[1:]))
res.raise_for_status()  
#simdi indirdiginiz HTML'den en iyi arama sonucu baglantilarini cikarmak icin "Beautiful Soup" u kullanmaniz gerekiyor.

# Retrieve top search result links.
soup = bs4.BeautifulSoup(res.text)   
 
# Open a browser tab for each result.
linkElems = soup.select('.r a')  #buradaki en onemli nokta burasi.Arama sonuclarinin getirildigi kisim <h3 class="r">.Bu bilgiye
# google da arama yapip sag tiklayip sayfayi incele(inspect element) e tiklayarak ulasabilirsin
#HTML text and then use the selector '.r a' to find all <a> elements that are within an element that has the r CSS class.

# Open a browser tab for each result.
linkElems = soup.select('.r a')
numOpen = min(5, len(linkElems))           #ilk bes sonucu getirecek
for i in range(numOpen):
    webbrowser.open('http://google.com' + linkElems[i].get('href'))   

        
    
    
    
    
    
    